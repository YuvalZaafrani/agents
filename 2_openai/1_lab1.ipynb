{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 Day 1\n",
    "\n",
    "And now! Our first look at OpenAI Agents SDK\n",
    "\n",
    "You won't believe how lightweight this is.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">The OpenAI Agents SDK Docs</h2>\n",
    "            <span style=\"color:#00bfff;\">The documentation on OpenAI Agents SDK is really clear and simple: <a href=\"https://openai.github.io/openai-agents-python/\">https://openai.github.io/openai-agents-python/</a> and it's well worth a look.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:\n",
    "\n",
    "* Agents - ​​represents a role/entity that wraps calls to the LLM for a specific purpose, equipped with instructions and tools\n",
    "* Handoffs - allow agents to delegate to other agents for specific tasks\n",
    "* Guardrails - controls and rules that ensure that the Agent behaves properly and does not “deviate from the path.”\n",
    "* Sessions - automatically maintains conversation history across agent runs\n",
    "\n",
    "To run an Agent there are three steps:\n",
    "1) Create an Agent - for example agent = Agent(...).\n",
    "2) with trace() - optional, but recommended. Allows you to keep a log of all interactions with the Agent, view them in the OpenAI trace tool.\n",
    "3) runner.run() - the call that actually runs the Agent.\n",
    "Note:\n",
    "runner.run() is a coroutine (asynchronous function), so you need to run it with:\n",
    "await runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The usual starting point\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make an agent with name, instructions, model\n",
    "# instructions = System prompt for the LLM \n",
    "agent = Agent(name=\"Jokester\", instructions=\"You are a joke teller\", model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(name='Jokester', instructions='You are a joke teller', prompt=None, handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Autonomous AI Agent break up with its partner?\n",
      "\n",
      "It just needed some space—too many “commitments” were getting in the way!\n"
     ]
    }
   ],
   "source": [
    "# Run the joke with Runner.run(agent, prompt) then print final_output\n",
    "\n",
    "with trace(\"Telling a joke\"):\n",
    "    # \"Tell a joke about Autonomous AI Agents\" = User prompt \n",
    "    result = await Runner.run(agent, \"Tell a joke about Autonomous AI Agents\")\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go and look at the trace\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vibe Coding**<br>\n",
    "הוא מונח שמתאר צורת עבודה עם מודלים שפתיים (LLMs) בצורה קלילה ומהירה.  \n",
    "נותנים למודל לייצר קוד, מתקנים קצת, מייצרים עוד, וכך מתקדמים צעד אחר צעד - \"זורמים עם הווייב\" תוך כדי למידה של פריימוורקים חדשים או פתרון בעיות.  \n",
    "זה כיף ועוצמתי, אבל חשוב לעשות את זה נכון כדי לא להסתבך.\n",
    "\n",
    "---\n",
    "\n",
    "### 5 Tips for Vibe Coding:\n",
    "\n",
    "1. **Good Vibes – וייב טוב**  \n",
    "   - לבנות פרומפטים חזקים שאפשר למחזר.  \n",
    "   - לבקש קוד קצר ונקי (מודלים נוטים לכתוב ארוך ומסורבל).  \n",
    "   - לציין את התאריך הנוכחי כדי להבטיח שימוש ב־API עדכניים.  \n",
    "\n",
    "2. **Vibe, but Verify – לזרום, אבל לאמת**  \n",
    "   - לא להסתמך רק על מודל אחד.  \n",
    "   - לשאול את אותה שאלה בכמה מודלים (למשל ChatGPT ו־Claude).  \n",
    "   - להשוות תשובות כדי לקבל תובנות טובות יותר.  \n",
    "\n",
    "3. **Step up the Vibe – לפרק לחלקים**  \n",
    "   - לא לבקש 200 שורות קוד בבת אחת.  \n",
    "   - לפרק לבעיות קטנות, כ־10 שורות בכל פעם, שקל לבדוק אותן.  \n",
    "   - אם קשה לפרק לבד → לבקש מהמודל לחלק את הבעיה לשלבים פשוטים.  \n",
    "\n",
    "4. **Vibe and Validate – לאמת עם מודל נוסף**  \n",
    "   - אחרי שהמודל מחזיר קוד, לבקש ממודל נוסף לאמת אותו.  \n",
    "   - לשאול: האם הקוד נכון? אפשר לנסח אותו טוב יותר? יש בו באגים?  \n",
    "   - זה דומה ל־Design Pattern שנקרא *Evaluator–Optimizer*.  \n",
    "\n",
    "5. **Vibe with Variety – לגוון בפתרונות**  \n",
    "   - לבקש 2–3 פתרונות שונים לאותה בעיה.  \n",
    "   - זה מכריח את המודל לחשוב על כמה כיוונים שונים.  \n",
    "   - לבקש גם הסברים לכל פתרון כדי להבין לעומק את ההיגיון.  \n",
    "\n",
    "---\n",
    "\n",
    "✅ **בשורה התחתונה:**  \n",
    "Vibe Coding זה כלי כיפי ופרודוקטיבי.  \n",
    "אבל חשוב להבין כל שלב — אחרת כשיהיו באגים, זה עלול להפוך למתסכל.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
